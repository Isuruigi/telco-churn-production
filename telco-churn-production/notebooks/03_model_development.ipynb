{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telco Customer Churn: Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers the model development process, including training baseline models, ensemble methods, hyperparameter tuning, and model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport yaml\n\n# Add src directory to path\nsys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n\n# Data and Preprocessing\nfrom src.data_loader import TelcoDataLoader\nfrom src.preprocessor import DataPreprocessor\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom src.base_model import LogisticRegressionModel\nfrom src.ensemble_models import RandomForestChurnModel, XGBoostChurnModel\nfrom src.advanced_ensemble import StackingChurnModel\n\n# Evaluation and Tuning\nfrom src.model_evaluator import ModelEvaluator\nfrom src.hyperparameter_tuner import HyperparameterTuner\nfrom src.cross_validation import CrossValidator\nfrom src.visualization import plot_feature_importance\n\n# Load config\nwith open('../config/config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\n# Load and preprocess data\nloader = TelcoDataLoader()\ndf = loader.load_raw_data()\ndf_processed = DataPreprocessor.preprocess_data(df.copy())\n\n# Create features and target\nX = df_processed.drop(config['target'], axis=1)\ny = df_processed[config['target']].apply(lambda x: 1 if x == 'Yes' else 0)\n\n# Create preprocessing pipeline\npreprocessor = DataPreprocessor().create_preprocessing_pipeline()\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=config['training']['random_state'], stratify=y)\n\n# Apply preprocessing\nX_train_transformed = preprocessor.fit_transform(X_train)\nX_test_transformed = preprocessor.transform(X_test)\n\n# Get feature names for later use\nfeature_names = preprocessor.get_feature_names_out()\nX_train_transformed = pd.DataFrame(X_train_transformed, columns=feature_names)\nX_test_transformed = pd.DataFrame(X_test_transformed, columns=feature_names)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Model (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "lr_model = LogisticRegressionModel(random_state=config['training']['random_state'])\n\nlr_model.train(X_train_transformed, y_train)\n\nlr_metrics = lr_model.evaluate(X_test_transformed, y_test)\nprint('Logistic Regression Metrics:', lr_metrics)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest\n\nrf_model = RandomForestChurnModel(random_state=config['training']['random_state'])\nrf_model.train(X_train_transformed, y_train)\nrf_metrics = rf_model.evaluate(X_test_transformed, y_test)\nprint('Random Forest Metrics:', rf_metrics)\n\n# XGBoost\n\nxgb_model = XGBoostChurnModel(random_state=config['training']['random_state'])\nxgb_model.train(X_train_transformed, y_train)\nxgb_metrics = xgb_model.evaluate(X_test_transformed, y_test)\nprint('XGBoost Metrics:', xgb_metrics)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Tuning (Example with RandomForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.ensemble import RandomForestClassifier\nparam_grid_rf = {\n    'n_estimators': [100, 200],\n    'max_depth': [10, 20],\n    'min_samples_leaf': [1, 2]\n}\n\nrf_tuner = HyperparameterTuner(RandomForestClassifier(random_state=config['training']['random_state']), param_grid_rf)\nbest_rf_model, best_params = rf_tuner.grid_search_cv(X_train_transformed, y_train)\n\n# Wrap the tuned model in our custom class\ntuned_rf = RandomForestChurnModel()\ntuned_rf.model = best_rf_model"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "models_to_compare = {\n    'Logistic Regression': lr_model.model,\n    'Random Forest': rf_model.model,\n    'XGBoost': xgb_model.model,\n    'Tuned Random Forest': tuned_rf.model\n}\n\nevaluator = ModelEvaluator(models_to_compare, X_test_transformed, y_test)\ncomparison_df = evaluator.compare_multiple_models()\nprint(comparison_df)\n\n# Create plots (ROC, Confusion Matrix)\nevaluator.create_evaluation_plots()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation of the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Let's assume Tuned Random Forest is our best model\ncv_validator = CrossValidator(tuned_rf.model, X_train_transformed, y_train)\ncv_validator.validate_model_stability()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "feature_importance = tuned_rf.get_feature_importance()\nif feature_importance is not None:\n    plot_feature_importance(feature_importance)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
