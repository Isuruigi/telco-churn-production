# Kafka Integration - Project Structure

Complete directory structure for the Telco Churn Prediction Kafka integration.

---

## ğŸ“ Directory Structure

```
Telco churn project 1/
â”‚
â”œâ”€â”€ kafka/                              â­ KAFKA INTEGRATION (Mini Project 2)
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                       # Configuration (Kafka, model, data paths)
â”‚   â”œâ”€â”€ producer.py                     # Kafka producer (streaming & batch)
â”‚   â”œâ”€â”€ consumer.py                     # Kafka consumer with ML predictions
â”‚   â”œâ”€â”€ test_pipeline.py                # Full pipeline testing script
â”‚   â”œâ”€â”€ requirements-kafka.txt          # Kafka-specific dependencies
â”‚   â”‚
â”‚   â”œâ”€â”€ README.md                       # Main Kafka integration documentation
â”‚   â”‚
â”‚   â”œâ”€â”€ airflow/                        # Airflow orchestration
â”‚   â”‚   â””â”€â”€ dags/
â”‚   â”‚       â”œâ”€â”€ kafka_streaming_dag.py  # Streaming pipeline DAG
â”‚   â”‚       â””â”€â”€ kafka_batch_dag.py      # Batch pipeline DAG
â”‚   â”‚
â”‚   â”œâ”€â”€ logs/                           # Application logs
â”‚   â”‚   â”œâ”€â”€ batch/                      # Batch run logs
â”‚   â”‚   â”‚   â”œâ”€â”€ producer_*.log
â”‚   â”‚   â”‚   â””â”€â”€ consumer_*.log
â”‚   â”‚   â””â”€â”€ streaming/                  # Streaming logs
â”‚   â”‚       â””â”€â”€ streaming_consumer.log
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                        # Generated reports
â”‚   â”‚   â””â”€â”€ batch/                      # Batch summary reports
â”‚   â”‚       â””â”€â”€ batch_report_*.md
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints/                    # Batch processing checkpoints
â”‚   â”‚   â””â”€â”€ producer_checkpoint_*.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ screenshots/                    # Documentation screenshots
â”‚   â”‚   â”œâ”€â”€ SCREENSHOTS_GUIDE.md        # Screenshot capture guide
â”‚   â”‚   â”œâ”€â”€ kafka_logs/                 # Kafka operation screenshots
â”‚   â”‚   â”‚   â”œâ”€â”€ producer_streaming.png
â”‚   â”‚   â”‚   â”œâ”€â”€ consumer_streaming.png
â”‚   â”‚   â”‚   â”œâ”€â”€ topic_messages.png
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ airflow_logs/               # Airflow DAG screenshots
â”‚   â”‚       â”œâ”€â”€ streaming_dag_graph.png
â”‚   â”‚       â”œâ”€â”€ batch_dag_graph.png
â”‚   â”‚       â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ docs/                           # Additional documentation
â”‚       â”œâ”€â”€ KAFKA_README.md             # Detailed Kafka documentation
â”‚       â”œâ”€â”€ QUICKSTART.md               # Quick start guide
â”‚       â”œâ”€â”€ TEST_RESULTS.md             # Validation report
â”‚       â””â”€â”€ AIRFLOW_SETUP_GUIDE.md      # Airflow setup guide
â”‚
â”œâ”€â”€ telco-churn-production/            # Mini Project 1 (Original ML project)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ sklearn_pipeline.pkl   # Trained ML model (used by consumer)
â”‚   â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ modeling/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv  # Dataset (used by producer)
â”‚   â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ requirements.txt                    # Full project dependencies
â”œâ”€â”€ docker-compose.yml                  # Kafka/Zookeeper Docker setup
â””â”€â”€ README.md                           # Main project README

```

---

## ğŸ”§ File Descriptions

### Core Kafka Files

#### **config.py**
- Central configuration for Kafka pipeline
- Kafka broker settings
- Topic names
- Model and data paths (points to telco-churn-production/)
- Streaming and batch configuration
- Logging setup

#### **producer.py**
- TelcoProducer class
- **Streaming mode**: Random sampling at configurable rate
- **Batch mode**: Sequential processing with checkpoints
- Sends messages to `telco-raw-customers` topic
- JSON serialization with timestamps
- Error handling to dead letter queue

#### **consumer.py**
- TelcoConsumer class
- Loads sklearn_pipeline.pkl model
- Consumes from `telco-raw-customers` topic
- **Streaming mode**: Real-time continuous processing
- **Batch mode**: Windowed processing with summaries
- Runs ML predictions
- Publishes to `telco-churn-predictions` topic
- Error handling to dead letter queue

#### **test_pipeline.py**
- Full pipeline testing
- Prerequisites verification
- Multi-threaded producer/consumer execution
- Health checks
- Prediction verification

---

### Airflow DAGs

#### **kafka_streaming_dag.py**
Location: `kafka/airflow/dags/`

**Purpose**: Manage long-running streaming consumer

**Tasks**:
1. Check Kafka connectivity
2. Verify topics exist
3. Verify model file
4. Start streaming consumer
5. Health check (every 5 minutes)
6. Monitor metrics
7. Cleanup on failure

**Paths**:
- KAFKA_DIR: `kafka/`
- PROJECT_ROOT: `Telco churn project 1/`
- Consumer script: `kafka/consumer.py`
- Logs: `kafka/logs/streaming_consumer.log`
- Model: `telco-churn-production/src/models/sklearn_pipeline.pkl`

#### **kafka_batch_dag.py**
Location: `kafka/airflow/dags/`

**Purpose**: Hourly batch processing with reports

**Schedule**: `0 * * * *` (every hour)

**Tasks**:
1. Check prerequisites
2. Run batch producer
3. Run batch consumer
4. Parse summary statistics
5. Generate markdown report
6. Check success threshold
7. Send notification
8. Cleanup

**Paths**:
- KAFKA_DIR: `kafka/`
- Producer script: `kafka/producer.py`
- Consumer script: `kafka/consumer.py`
- Logs: `kafka/logs/batch/`
- Reports: `kafka/reports/batch/`
- Model: `telco-churn-production/src/models/sklearn_pipeline.pkl`
- Data: `telco-churn-production/data/WA_Fn-UseC_-Telco-Customer-Churn.csv`

---

### Documentation Files

#### **README.md** (kafka/)
Main Kafka integration documentation
- Overview and architecture
- Features and capabilities
- Installation instructions
- Usage examples
- Airflow DAG documentation
- Monitoring and troubleshooting

#### **docs/KAFKA_README.md**
Detailed Kafka-specific documentation
- Message schemas
- Topic management
- Consumer groups
- Performance tuning

#### **docs/QUICKSTART.md**
Quick start guide
- 3 ways to run the pipeline
- Common commands
- Example sessions

#### **docs/TEST_RESULTS.md**
Test validation report
- Test execution results
- Performance metrics
- Success criteria

#### **docs/AIRFLOW_SETUP_GUIDE.md**
Complete Airflow setup guide
- Installation steps
- DAG deployment
- Configuration
- Monitoring
- Troubleshooting

#### **screenshots/SCREENSHOTS_GUIDE.md**
Screenshot capture instructions
- 16 required screenshots
- Commands to run
- What to capture
- Naming conventions

---

## ğŸ”€ Path References

### From Kafka Scripts

**config.py:**
```python
BASE_DIR = Path(__file__).parent          # kafka/
PROJECT_ROOT = BASE_DIR.parent            # Telco churn project 1/
MODEL_PATH = PROJECT_ROOT / 'telco-churn-production' / 'src' / 'models' / 'sklearn_pipeline.pkl'
DATA_PATH = PROJECT_ROOT / 'telco-churn-production' / 'data' / 'WA_Fn-UseC_-Telco-Customer-Churn.csv'
```

**producer.py, consumer.py:**
```python
import config  # Imports from kafka/config.py
```

### From Airflow DAGs

**Both DAGs:**
```python
KAFKA_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
# Result: kafka/

PROJECT_ROOT = os.path.abspath(os.path.join(KAFKA_DIR, '..'))
# Result: Telco churn project 1/

sys.path.insert(0, KAFKA_DIR)
# Can now import config, etc.
```

---

## ğŸš€ Running from Different Locations

### From `kafka/` directory:

```bash
cd kafka

# Run producer
python producer.py --mode streaming

# Run consumer
python consumer.py --mode streaming

# Test pipeline
python test_pipeline.py --verify-only
```

### From project root:

```bash
cd "Telco churn project 1"

# Run with full paths
python kafka/producer.py --mode streaming
python kafka/consumer.py --mode streaming
python kafka/test_pipeline.py --verify-only
```

### Airflow (from anywhere):

```bash
# Copy DAGs
cp kafka/airflow/dags/*.py $AIRFLOW_HOME/dags/

# Trigger from CLI
airflow dags trigger kafka_streaming_pipeline
airflow dags trigger kafka_batch_pipeline
```

---

## ğŸ“Š Data Flow

```
telco-churn-production/data/
    â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
            â†“
        (read by)
            â†“
    kafka/producer.py
            â†“
        (sends to)
            â†“
    Kafka Topic: telco-raw-customers
            â†“
        (consumed by)
            â†“
    kafka/consumer.py
            â†“
        (loads model)
            â†“
    telco-churn-production/src/models/sklearn_pipeline.pkl
            â†“
        (generates predictions)
            â†“
    Kafka Topic: telco-churn-predictions
            â†“
        (logged to)
            â†“
    kafka/logs/batch/ or kafka/logs/streaming/
```

---

## âœ… Installation Checklist

- [ ] Kafka running on localhost:29092
- [ ] Topics created (or auto-create enabled)
- [ ] Model file exists: `telco-churn-production/src/models/sklearn_pipeline.pkl`
- [ ] Data file exists: `telco-churn-production/data/WA_Fn-UseC_-Telco-Customer-Churn.csv`
- [ ] Python dependencies installed: `pip install -r requirements.txt`
- [ ] Directory structure created: `logs/`, `reports/`, `checkpoints/`
- [ ] Airflow installed (optional): `pip install apache-airflow==2.7.0`
- [ ] DAGs deployed (optional): Copied to `$AIRFLOW_HOME/dags/`

---

## ğŸ“ Notes

1. **All Kafka files are self-contained in `kafka/` directory**
2. **Airflow DAGs correctly reference kafka/ paths**
3. **Model and data remain in original `telco-churn-production/` location**
4. **Logs, reports, checkpoints stored in `kafka/` subdirectories**
5. **Screenshots documented with capture instructions**
6. **Documentation organized in `kafka/docs/`**

---

*Last Updated: October 16, 2025*
